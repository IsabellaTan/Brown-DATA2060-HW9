{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Homework 9**\n",
    "\n",
    "Due: **December 5, 5pm** (late submission until December 8, 5pm -- no submission possible afterwards)\n",
    "\n",
    "K Means Coding Assignment: 15 points\n",
    "\n",
    "K Means Project Report: 10 points\n",
    "\n",
    "PCA Coding Assignment: 15 points\n",
    "\n",
    "PCA Project Report: 10 points\n",
    "\n",
    "### Name: [TODO]\n",
    "\n",
    "### Link to the github repo: [TODO]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Run the evironment test below, make sure you get all green check, if not, you will lose 2 points for each red flag.\n",
    "\n",
    "This is the original DATA2060 conda environment, not the one you used in the previous home work!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "from packaging.version import parse as Version\n",
    "from platform import python_version\n",
    "\n",
    "OK = '\\x1b[42m[ OK ]\\x1b[0m'\n",
    "FAIL = \"\\x1b[41m[FAIL]\\x1b[0m\"\n",
    "\n",
    "try:\n",
    "    import importlib\n",
    "except ImportError:\n",
    "    print(FAIL, \"Python version 3.12.11 is required,\"\n",
    "                \" but %s is installed.\" % sys.version)\n",
    "\n",
    "def import_version(pkg, min_ver, fail_msg=\"\"):\n",
    "    mod = None\n",
    "    try:\n",
    "        mod = importlib.import_module(pkg)\n",
    "        if pkg in {'PIL'}:\n",
    "            ver = mod.VERSION\n",
    "        else:\n",
    "            ver = mod.__version__\n",
    "        if Version(ver) == Version(min_ver):\n",
    "            print(OK, \"%s version %s is installed.\"\n",
    "                  % (lib, min_ver))\n",
    "        else:\n",
    "            print(FAIL, \"%s version %s is required, but %s installed.\"\n",
    "                  % (lib, min_ver, ver))    \n",
    "    except ImportError:\n",
    "        print(FAIL, '%s not installed. %s' % (pkg, fail_msg))\n",
    "    return mod\n",
    "\n",
    "\n",
    "# first check the python version\n",
    "pyversion = Version(python_version())\n",
    "\n",
    "if pyversion >= Version(\"3.12.11\"):\n",
    "    print(OK, \"Python version is %s\" % pyversion)\n",
    "elif pyversion < Version(\"3.12.11\"):\n",
    "    print(FAIL, \"Python version 3.12.11 is required,\"\n",
    "                \" but %s is installed.\" % pyversion)\n",
    "else:\n",
    "    print(FAIL, \"Unknown Python version: %s\" % pyversion)\n",
    "\n",
    "    \n",
    "print()\n",
    "requirements = {'matplotlib': \"3.10.5\", 'numpy': \"2.3.2\",'sklearn': \"1.7.1\", \n",
    "                'pandas': \"2.3.2\", 'pytest': \"8.4.1\", 'torch':\"2.7.1\"}\n",
    "\n",
    "# now the dependencies\n",
    "for lib, required_version in list(requirements.items()):\n",
    "    import_version(lib, required_version)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1: K-means Clustering (15 points)\n",
    "\n",
    "#### Introduction\n",
    "\n",
    "In this assignment, you will implement an iterative method for\n",
    "clustering: k-means. Your implementation will be used for a k-means\n",
    "classifier, which will be trained and tested on handwritten digits\n",
    "dataset to classify an exact digit (0 to 9).\n",
    "\n",
    "#### Stencil Code & Data\n",
    "\n",
    "You have been provided with the following stencil files:\n",
    "\n",
    "-   `Models`: contains the K-means classifier class that you will\n",
    "    need to fill in.\n",
    "\n",
    "-   `Check Model`: contains a series of tests to ensure you are coding your \n",
    "    model properly.\n",
    "\n",
    "-   `Main`: contains a main function to read data, run classifier and\n",
    "    print/visualize results.\n",
    "\n",
    "-   `kmeans`: contains helper functions for K-means clustering via\n",
    "    iterative improvement that you will need to fill in.\n",
    "\n",
    "#### 8x8 Hand-written digits\n",
    "\n",
    "In the `digits.csv` file, each row is an observation of a 8 x 8\n",
    "hand-written digit (0 - 9), containing a label in the first column and 8\n",
    "x 8 = 64 features (pixel values) in the rest of columns. We will ignore\n",
    "the label in this exercise.\n",
    "\n",
    "#### Data Format\n",
    "\n",
    "We have written all the preprocessing code for you. The dataset is\n",
    "represented by a `namedtuple` with one field:\n",
    "\n",
    "-   `data.inputs` is a $m \\times p$ NumPy array that contains the binary\n",
    "    features of the $m$ examples, where $p$ is the number of pixels in\n",
    "    each example (64).\n",
    "\n",
    "You can find more infomation on `namedtuple`\n",
    "[**here**](https://docs.python.org/3/library/collections.html#collections.namedtuple)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **K-means Clustering**\n",
    "\n",
    "K-means is a clustering algorithm most often used for unsupervised\n",
    "machine learning. In unsupervised learning, the learner is given a\n",
    "dataset with no labels and attempts to learn some useful representation\n",
    "of the dataset. \n",
    "\n",
    "-   You will run K-means clustering on the unlabeled training data and\n",
    "    plot the pixel representations of different cluster centers\n",
    "    (centroids): $M = \\{\\mu_1 \\dots \\mu_{10}\\}$, for clusters\n",
    "    $C=\\{C_{1} \\dots C_{10}\\}$. With K = 10, these cluster centers\n",
    "    should vaguely resemble the 10 digits (0-9).\n",
    "\n",
    "-   To predict the cluster ID, $C_{i,m+1}$ of a new datapoint ${\\bf x}_{m+1}$,\n",
    "    find the cluster center nearest to ${\\bf x}_{m+1}$, $\\mu_i$.\n",
    "\n",
    "*Note:* You don't need to worry about changing your centroid assignments\n",
    "in between runs, as we've set the random seed in the stencil."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Functions\n",
    "\n",
    "1.  `Models`\\\n",
    "    In this file, you will implement two functions. They are:\n",
    "\n",
    "    -   `KmeansClassifier`\n",
    "\n",
    "        -   **train()**: Learn K=10 cluster centroids (representatives)\n",
    "            from the data that are robust (because they are estimated\n",
    "            using a lot of data). Store cluster centroids as a Numpy\n",
    "            array in *model attribute*\n",
    "\n",
    "        -   **predict()**: predict cluster ID of inputs using the\n",
    "            closest centroid's assignment\n",
    "\n",
    "2.  `kmeans`:\\\n",
    "    In this file, you will implement four functions:\n",
    "\n",
    "    -   **init_centroids()**: pick **K** random data points as cluster\n",
    "        centers called centroids.\n",
    "\n",
    "    -   **assign_step()**: assign each data instance to its nearest\n",
    "        cluster centroid using Euclidean distance measure.\n",
    "\n",
    "    -   **update_step()**: find the new cluster centroids by taking the\n",
    "        average of its assigned data points.\n",
    "\n",
    "    -   **kmeans()**: run the $K$-means algorithm: initialize centroids,\n",
    "        then repeat the assignment step and update step until the\n",
    "        proportion the centroids \\[defined below\\] change between two\n",
    "        iterations is below a tolerance threshold or the maximum\n",
    "        iteration time is met. The tolerance threshold is passed into\n",
    "        `kmeans()` as `tol` and tolerance is compared against the ratio\n",
    "        of the norm of the difference between centroids and the norm of\n",
    "        the original centroids.\n",
    "\n",
    "    *Note:* You might also want to create a separate function that\n",
    "    calculates the Euclidean distance between two data points in the\n",
    "    `kmeans` file. Please feel free to do so.\n",
    "\n",
    "3.  `main`:\\\n",
    "    You will not need to implement any functions in this file. However,\n",
    "    you will need to do two things:\n",
    "\n",
    "    -   Uncomment the call to `plot_Kmeans` in main. This function will\n",
    "        allow you to see the centroids that your k-means model learns."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Kmeans**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "\n",
    "def init_centroids(k, inputs):\n",
    "    \"\"\"\n",
    "    Selects k random rows from inputs and returns them as the chosen centroids\n",
    "    Hint: use random.sample (it is already imported for you!)\n",
    "    :param k: number of cluster centroids\n",
    "    :param inputs: a 2D Numpy array, each row of which is one input\n",
    "    :rand: random seed to be used when sampling from inputs\n",
    "    :return: a Numpy array of k cluster centroids, one per row\n",
    "    \"\"\"\n",
    "    row_indeices = random.sample(range(inputs.shape[0]), k)\n",
    "    return inputs[row_indeices]\n",
    "\n",
    "\n",
    "def assign_step(inputs, centroids):\n",
    "    \"\"\"\n",
    "    Determines a centroid index for every row of the inputs using Euclidean Distance\n",
    "    :param inputs: inputs of data, a 2D Numpy array\n",
    "    :param centroids: a Numpy array of k current centroids\n",
    "    :return: a Numpy array of centroid indices, one for each row of the inputs\n",
    "    \"\"\"\n",
    "    # TODO\n",
    "\n",
    "    return \n",
    "\n",
    "\n",
    "def update_step(inputs, indices, k):\n",
    "    \"\"\"\n",
    "    Computes the centroid for each cluster\n",
    "    :param inputs: inputs of data, a 2D Numpy array\n",
    "    :param indices: a Numpy array of centroid indices, one for each row of the inputs\n",
    "    :param k: number of cluster centroids, an int\n",
    "    :return: a Numpy array of k cluster centroids, one per row\n",
    "    \"\"\"\n",
    "    # TODO\n",
    "\n",
    "    return \n",
    "\n",
    "\n",
    "def kmeans(inputs, k, max_iter, tol):\n",
    "    \"\"\"\n",
    "    Runs the K-means algorithm on n rows of inputs using k clusters via iterative improvement\n",
    "    :param inputs: inputs of data, a 2D Numpy array\n",
    "    :param k: number of cluster centroids, an int\n",
    "    :param max_iter: the maximum number of times the algorithm can iterate trying to optimize the centroid values, an int\n",
    "    :param tol: the tolerance we determine convergence with when compared to the ratio as stated on handout\n",
    "    :param rand: a given random seed to be used within init_centroids\n",
    "    :return: a Numpy array of k cluster centroids, one per row\n",
    "    \"\"\"\n",
    "    # TODO\n",
    "\n",
    "    return "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Models**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class KmeansClassifier(object):\n",
    "    \"\"\"\n",
    "    K-Means Classifier via Iterative Improvement\n",
    "    @attrs:\n",
    "        k: The number of clusters to form as well as the number of centroids to\n",
    "           generate (default = 10), an int\n",
    "        tol: Value specifying our convergence criterion. If the ratio of the\n",
    "             distance each centroid moves to the previous position of the centroid\n",
    "             is less than this value, then we declare convergence.\n",
    "        max_iter: the maximum number of times the algorithm can iterate trying\n",
    "                  to optimize the centroid values, an int,\n",
    "                  the default value is set to 500 iterations\n",
    "        cluster_centers_: a Numpy array where each element is one of the k cluster centers\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, n_clusters = 10, max_iter = 500, threshold = 1e-6):\n",
    "        \"\"\"\n",
    "        Initiate K-Means with some parameters\n",
    "        \"\"\"\n",
    "        self.k = n_clusters\n",
    "        self.tol = threshold\n",
    "        self.max_iter = max_iter\n",
    "        self.cluster_centers_ = np.array([])\n",
    "\n",
    "    def train(self, X):\n",
    "        \"\"\"\n",
    "        Compute K-Means on each cluster and store your result in self.cluster_centers_\n",
    "        :param X: inputs of training data, a 2D Numpy array\n",
    "        :param rand: random seed to be used during training\n",
    "        :return: None\n",
    "        \"\"\"\n",
    "        # TODO (hint: use kmeans())\n",
    "\n",
    "    \n",
    "    def predict(self, X):\n",
    "        \"\"\"\n",
    "        Predicts the cluster ID of each sample in X.\n",
    "        :param X: A dataset as a 2D Numpy array\n",
    "        :return: A Numpy array of predicted cluster IDs\n",
    "        \"\"\"\n",
    "\n",
    "        # TODO: complete this step only after having plotted the centroids!\n",
    "\n",
    "        return "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Check Models**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pytest\n",
    "from collections import namedtuple\n",
    "np.random.seed(0)\n",
    "random.seed(0)\n",
    "\n",
    "# Creates Test Model with 3 clusters\n",
    "test_model1 = KmeansClassifier(3)\n",
    "# Creates Test Model with 2 clusters\n",
    "test_model2 = KmeansClassifier(2)\n",
    "\n",
    "# Creates Test Data\n",
    "x = np.array([[0,1,7], [1,1,9], [5,0,1], [4,1,1], [0,5,0], [1,9,0]])\n",
    "x2 = np.array([[3,1,7], [5,1,9], [2,8,1], [0,1,1], [0,5,0], [2,0,8]])\n",
    "\n",
    "# Test Train Model and Checks Cluster Centers\n",
    "test_model1.train(x)\n",
    "test_model2.train(x2)\n",
    "test_model1_sorted_clusters = test_model1.cluster_centers_[test_model1.cluster_centers_[:, 0].argsort()]\n",
    "test_model2_sorted_clusters = test_model2.cluster_centers_[test_model2.cluster_centers_[:, 0].argsort()]\n",
    "assert (test_model1_sorted_clusters == np.array([[.5, 7, 0], [.5, 1, 8], [4.5, .5, 1]])).all()\n",
    "assert (test_model2_sorted_clusters == np.array([[1.25, 1.75, 4], [3.5, 4.5, 5]])).all()\n",
    "\n",
    "# Tests Model Predict\n",
    "assert (np.sort(test_model1.predict(x)) == [0, 0, 1, 1, 2, 2]).all()\n",
    "assert (np.sort(test_model2.predict(x2)) == [0, 0, 1, 1, 1, 1]).all()\n",
    "\n",
    "# TODO: student should print their names and date\n",
    "print('student name and date ')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Main**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.colors import ListedColormap\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "## KMEANS HELPERS ##\n",
    "def plot_Kmeans(model):\n",
    "    \"\"\"\n",
    "        Takes in a pre-trained K-Means classifier model and plots the 10 centroids.\n",
    "        Note: this function is designed only for the digits.csv data set.\n",
    "    :param model: pre-trained K-Means classifier model object\n",
    "    :return: None\n",
    "    \"\"\"\n",
    "    if isinstance(model, KmeansClassifier) == False:\n",
    "        print(\"Invalid input! Model must be a KmeansClassifier object.\")\n",
    "        return\n",
    "\n",
    "    cluster_centers = model.cluster_centers_\n",
    "    fig, ax = plt.subplots(1, len(cluster_centers), figsize=(6, 2))\n",
    "\n",
    "    for i in range(len(cluster_centers)):\n",
    "        axi = ax[i]\n",
    "        center = cluster_centers[i]\n",
    "        center = np.array(center).reshape(8,8)\n",
    "        axi.set(xticks=[], yticks=[])\n",
    "        axi.imshow(center, interpolation='nearest', cmap=plt.cm.binary)\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "\n",
    "def runKMeans():\n",
    "    '''\n",
    "        Trains, plots, and tests K-Means classifier on digits.csv dataset.\n",
    "    '''\n",
    "    NUM_CLUSTERS = 10 # Change only for Question 3 of the Project Report\n",
    "    random.seed(1) # DO NOT CHANGE\n",
    "    np.random.seed(1) # DO NOT CHANGE\n",
    "\n",
    "\n",
    "    # Read data\n",
    "    data = pd.read_csv(\"digits.csv\", header = None)\n",
    "\n",
    "    # Features columns are indexed from 1 to the end, make sure that dtype = float32\n",
    "    inputs = data.values[:, 1:].astype(\"float32\")\n",
    "\n",
    "    # Split data into training set and test set with a ratio of 2:1\n",
    "    train_inputs, test_inputs = train_test_split(inputs, test_size = 0.33)\n",
    "\n",
    "    train_data = train_inputs\n",
    "    test_data = test_inputs\n",
    "    print(\"Shape of training data inputs: \", train_data.shape)\n",
    "    print(\"Shape of test data inputs:\", test_data.shape)\n",
    "\n",
    "    # Train K-Means Classifier\n",
    "    kmeans_model = KmeansClassifier(NUM_CLUSTERS)\n",
    "    kmeans_model.train(train_data)\n",
    "\n",
    "    # DO NOT MODIFY ABOVE THIS LINE!\n",
    "\n",
    "    # plot_Kmeans(kmeans_model)\n",
    "\n",
    "\n",
    "# DO NOT MODIFY BELOW\n",
    "runKMeans()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Project Report** (10 points)\n",
    "\n",
    "#### **Question 1**\n",
    "Display your output of `plot_Kmeans()`. Does your plot match your expectations?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Question 2**\n",
    "In this assignment, you implemented k-means through a Euclidean\n",
    "distance metric. Describe other distance metrics that can be used\n",
    "and how they cluster inputs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Question 3**\n",
    "In `runKMeans()` in the Main section, change the number of clusters (`NUM_CLUSTERS`) to 6, and display the digits with `plot_Kmeans(kmeans_model)`. Do this as well for 15 clusters. Describe what the clutsers' centers (centroids) look like and why this is happening."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2: Principal Component Analysis\n",
    "\n",
    "### **Mathematical Framework**\n",
    "\n",
    "Principal Component Analysis is a popular dimensionality reduction method that is used to analyze, interpret and visualize high dimensional data. Its simplicity and non-parametric nature make it applicable in varied settings - biology, computer graphics and atmospheric science, to name a few.\n",
    "\n",
    "At a high level the goal of PCA is to identify the most meaningful basis to re-express a dataset. The hope is that this new basis will filter out the noise and reveal a hidden structure. The mathematical setup is as follows:\n",
    "\n",
    "Let $\\mathbf{x}_1, \\mathbf{x}_2, \\cdots \\mathbf{x}_m$ be vectors $\\in \\mathbb{R}^{d}$; our goal is to find a suitable linear transformation that reduces the dimensionality to $\\mathbb{R}^{n}$, where $n<d$. So the first key idea is **linearity**- that is, is there another basis, which is within the span of the original basis, that best re-expresses our data set?\n",
    "\n",
    "But what do we mean by re-express? And how do we know what's the best way to re-express our dataset? The answer here depends on what we want to accomplish from dimensionality reduction. Very briefly though, we'd like for PCA to identify dimensions that reduce noise. Noise here is not an absolute scale but relative to the signal strength. A common measure is the *signal-to-noise ratio* (SNR), or a ratio of variances: $SNR = \\dfrac{\\sigma_{signal}^{2}}{\\sigma_{noise}^{2}}$\n",
    "\n",
    "![Signal vs. Noise](PCA-signal-graph.png)\n",
    "\n",
    "Another good to have, when we're re-expressing data, is reduced redundancy. Highly correlated (high covariance) features don't add much value when the goal is to reduce dimensionality. Note that the correlation may be positive or negative; we only care about the absolute value here.\n",
    "\n",
    "This brings us to the second key idea **variance and covariance**. Features which demonstrate high variance are interesting, and will likely help uncover a useful structure. And feature pairs with high covariance imply high redundancy. The mathematical formulation is as follows:\n",
    "\n",
    "Let $\\mathbf{X} \\in \\mathbb{R}^{m \\times d}$ represent the original dataset ($m$ is the number of training points and $d$ is the number of features) such that each feature has a **mean of zero** ($\\mu_{i} = 0$ $\\forall i \\in \\{1, 2, ... d\\}$) then the covariance matrix is:\n",
    "$$\\mathbf{C_{X}} = \\frac{1}{m}\\mathbf{X}^{T}\\mathbf{X}$$\n",
    "\n",
    "Note that $\\mathbf{C_{X}}$ is a square symmetric matrix of size $d \\times d$, where the diagonal elements are features variances and off-diagonal values are covariances. Also, in practice since we're estimating sample variance the $\\frac{1}{m}$ is actually $\\frac{1}{(m-1)}$; but the idea remains the same. Circling back to the idea that high variance and low covariance is favorable, we can say that our covariance matrix is optimal if (let us assume $\\mathbf{Y}$ is the transformed data $\\mathbf{C_{Y}}$ is the optimal covariance matrix):\n",
    "1. All of the diagonal terms of the covariance matrix ($\\mathbf{C_Y}$) are zero.\n",
    "2. Each successive dimension in the transformed data ($\\mathbf{Y}$) should be rank ordered according to variance.\n",
    "\n",
    "Turns out that the solution to PCA is based on the eigenvector decomposition of the covariance matrix of the original data, $\\mathbf{C_X}$.\n",
    "\n",
    "Let's tie all of this together. So our dataset $\\mathbf{X} \\in \\mathbb{R}^{m \\times d}$ is in a high-dimensional feature space; we hope to find a linear transformation $\\mathbf{W} \\in \\mathbb{R}^{d \\times n}$ ($n < d$) such that $\\mathbf{Y} = \\mathbf{X}\\mathbf{W} \\in \\mathbb{R}^{m \\times n}$, and $\\mathbf{C_Y} = \\frac{1}{m}\\mathbf{Y}^T\\mathbf{Y}$ is a diagonal matrix, then the columns of $\\mathbf{W}$ are the principal components. Then setting the columns of $\\mathbf{W}$ as the eigenvectors of $\\mathbf{C}_{X}$ is the solution to our problem.\n",
    "\n",
    "**Proof**\n",
    "\n",
    "\n",
    "$\\begin{align}\n",
    "\\mathbf{C_Y} &= \\frac{1}{m}\\mathbf{Y}^T\\mathbf{Y} \\\\ \n",
    "&= \\frac{1}{m}(\\mathbf{XW})^T(\\mathbf{XW}) \\\\\n",
    "&= \\frac{1}{m}\\mathbf{W}^T\\mathbf{C_X}\\mathbf{W}\n",
    "\\end{align}$\n",
    "\n",
    "Since $\\mathbf{C_X}$ is a symmetric matrix we can decompose it using the Eigendecomposition:\n",
    "\n",
    "$\\begin{align}\n",
    "\\mathbf{C_X} &= \\mathbf{EDE}^T \\\\ \n",
    "\\end{align}$\n",
    "\n",
    "Now, here lies the trick. We can pick $\\mathbf{W}$ such that, $\\mathbf{W} = \\mathbf{E}$. Note that $\\mathbf{E}$ is guaranteed to be orthonormal, i.e. $\\mathbf{E}^T\\mathbf{E} = \\mathbf{I}$.\n",
    "\n",
    "$\\begin{align}\n",
    "\\implies \\mathbf{C_Y} &= \\mathbf{W}^T\\mathbf{EDE}^T\\mathbf{W} \\\\ \n",
    "&= \\mathbf{E}^T\\mathbf{EDE}^T\\mathbf{E} \\\\\n",
    "&= \\mathbf{D}\n",
    "\\end{align}$\n",
    "\n",
    "Therefore, setting the columns of $\\mathbf{W}$ as the eigenvectors of $\\mathbf{C}_{X}$ is the solution."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Coding PCA** (15 points)\n",
    "\n",
    "Complete the following functions:\n",
    "\n",
    "1. **`fit`** - primary method that takes input data `X_init` and calculates the feature means, components and variance share of each component. Make sure to use the `np.linalg.eigh` for the eigendecomposition; other methods may give inconsistent results for the test cases. In the discussion above we assumed that the mean of each feature is zero-- **center the data before finding the components.** The variance share of a component is the ratio of its eigenvalue and the sum of all eigenvalues.\n",
    "\n",
    "\n",
    "2. **`transform`** - transform data `X` basis the components learnt from `X_init`. Make sure to center `X` based on the feature means calculated in `fit`.\n",
    "\n",
    "\n",
    "3. **`inverse`** - inverse of the transform. If $\\mathbf{Y} = \\mathbf{X}\\mathbf{W}$ is the transformed data then $\\mathbf{\\tilde{X}} = \\mathbf{Y}\\mathbf{W}^T$ is the inverse transform. Here $\\mathbf{W}^T$ can be thought of as the recovery matrix which converts the data back to the original feature space. Remember we center the data in `fit`; what would be the inverse of centering?\n",
    "\n",
    "\n",
    "4. **`mean_squared_error`** - If $\\mathbf{Y}$ is the transformed data and $\\mathbf{\\tilde{X}}$ is the inverse of $\\mathbf{Y}$, the squared mean squared error $\\frac{1}{m}\\sum_{i=1}^{m}\\lvert\\lvert\\mathbf{\\tilde{x}}_{i}-\\mathbf{x}_{i}\\rvert\\rvert_{2}^{2}$ where $\\mathbf{\\tilde{x}}_{i}$ and $\\mathbf{x}_{i}$ are the rows of $\\mathbf{\\tilde{X}}$ and $\\mathbf{X}$ respectively."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "class PCA:\n",
    "\n",
    "    def __init__(self):\n",
    "        \"\"\"Initialize PCA.\"\"\"\n",
    "        \n",
    "        # Assign appropriate values to these instance variables in the fit method.\n",
    "        \n",
    "        self.ftr_means = None\n",
    "        self.cov_mat = None\n",
    "        self.components = None\n",
    "        self.variance_share = None\n",
    "        \n",
    "    def fit(self, X_init:np.ndarray) -> None:\n",
    "        \"\"\"Fit the model with X_init.\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "          X_init: np.ndarray of shape (m x d); m = number of samples and d = number of features.\"\"\"\n",
    "        \n",
    "        # TODO: find feature means\n",
    "        \n",
    "        # TODO: center the data\n",
    "        \n",
    "        # TODO: find covariance matrix and perform eigendecomposition using np.linalg.eigh\n",
    "        \n",
    "        # TODO: sort components in descending order of eigenvalues. \n",
    "        # Calculate variance share of each component and store in a numpy array.\n",
    "\n",
    "        \n",
    "    def transform(self, X:np.ndarray, n_components:int) -> np.ndarray:\n",
    "        \"\"\"Apply dimensionality reduction to X (m x d).\n",
    "        Return transformed values with dimension n<d.\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "          X: np.ndarray of shape (m x d); m = number of samples and d = number of features.\n",
    "          n_components: Number of principal components, i.e. reduced dimension.\"\"\"\n",
    "        \n",
    "        assert n_components<=X.shape[1], \"Number of components cannot be greater than input dimension.\"\n",
    "        \n",
    "        # TODO: center X and transform as per number of components\n",
    "        \n",
    "        return \n",
    "\n",
    "    def inverse(self, X_dash:np.ndarray) -> np.ndarray:\n",
    "        \"\"\"Find the inverse transform of X_dash.\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "          X_dash: np.ndarray of shape (m x n); m = number of samples and n = number of features in the reduced space.\"\"\"\n",
    "        \n",
    "        # TODO\n",
    "\n",
    "        return \n",
    "    \n",
    "    def mean_squared_error(self, X:np.ndarray, n_components:int) -> float:\n",
    "        \"\"\"Apply dimensionality reduction to X (m x d). \n",
    "        Return error after taking the inverse of the transform.\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "          X: np.ndarray of shape (m x d); m = number of samples and d = number of features.\n",
    "          n_components: int Number of principal components, i.e. reduced dimension.\"\"\"\n",
    "        \n",
    "        assert n_components<=X.shape[1], \"Number of components cannot be greater than input dimension.\"\n",
    "        \n",
    "        # TODO\n",
    "\n",
    "        return "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Check Model**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DO NOT EDIT\n",
    "\n",
    "from pytest import approx\n",
    "\n",
    "model1 = PCA()\n",
    "model2 = PCA()\n",
    "\n",
    "X1_init = np.array([[7, 4, 2],[4, 5, 1],[5, 4, 1],[8, 4, 2],[7, 5, 0]])\n",
    "\n",
    "X2_init = np.array([[24,  5,  5,  0],[20,  6,  5,  0],[ 4,  4,  8,  0],[14,  6,  6,  2],\n",
    "                    [19,  5,  8,  0],[ 9,  5,  7,  1],[16,  3, 10,  1],[15,  5,  6,  2],[15,  6, 11,  0]])\n",
    "\n",
    "model1.fit(X1_init)\n",
    "model2.fit(X2_init)\n",
    "\n",
    "############# CHECK FIT ################\n",
    "\n",
    "def check_fit_dtypes(model, X):\n",
    "    assert isinstance(model.ftr_means, np.ndarray) and model.ftr_means.shape==(X.shape[1],), \"Feature means are of the incorrect type or shape.\"\n",
    "    assert isinstance(model.cov_mat, np.ndarray) and model.cov_mat.shape==(X.shape[1],X.shape[1]), \"Covariance matrix is of the incorrect type or shape.\"\n",
    "    assert isinstance(model.components, np.ndarray) and model.components.shape==(X.shape[1],X.shape[1]), \"Components matrix is of the incorrect type or shape.\"\n",
    "    assert isinstance(model.variance_share, np.ndarray) and model.variance_share.shape==(X.shape[1],), \"Variance share array is of the incorrect type or shape.\"\n",
    "    \n",
    "def check_vals(test_values, true_values, rtol=1e-2):\n",
    "    assert np.all(np.isclose(test_values,true_values,rtol=rtol))\n",
    "\n",
    "\n",
    "check_fit_dtypes(model1, X1_init)\n",
    "check_vals(model1.ftr_means, np.array([6.2, 4.4, 1.2])) # check feature means\n",
    "check_vals(model1.cov_mat, np.array([[ 2.7, -0.35, 0.45],[-0.35, 0.3, -0.35],[0.45, -0.35, 0.7]])) # check covariance\n",
    "check_vals(model1.components, np.array([[-0.9606, 0.2757, 0.0347],[0.1619, 0.4537, 0.8763],[-0.2258, -0.8474, 0.4805]]))\n",
    "check_vals(model1.variance_share, np.array([0.77426, 0.20026, 0.025438]))\n",
    "\n",
    "\n",
    "check_vals(model2.ftr_means, np.array([15.1111,  5.0000,  7.3333,  0.6666]))\n",
    "check_vals(model2.cov_mat,np.array([[35.111,1.625,-3.9166,-0.95833],[1.625,1.,-0.75,0.],\n",
    "                                    [-3.9166,-0.75,4.5,-0.375],[-0.95833333,0.,-0.375,0.75]]))\n",
    "\n",
    "check_vals(model2.components,np.array([[-0.9906,-0.1268,-0.0359,-0.0372],[-0.0491,0.1641,0.9673,-0.1868],\n",
    "                                       [0.1252,-0.968,0.138,-0.1684],[0.0258,0.1417,-0.2095,-0.9671]]))\n",
    "check_vals(model2.variance_share, np.array([0.8634, 0.1008, 0.0201, 0.0157]))\n",
    "\n",
    "\n",
    "############# CHECK TRANSFORM ################\n",
    "\n",
    "def check_transform_dtypes(model, X, n_components=2):\n",
    "    Y = model.transform(X, n_components)\n",
    "    assert isinstance(Y, np.ndarray) and Y.shape==(X.shape[0],n_components)\n",
    "\n",
    "X1 = np.array([[4,3,2],[2,5,0],[4,4,2]])\n",
    "X2 = np.array([[18,4,3,2],[10,7,9,2],[16,4,9,1],[3,5,4,1],\n",
    "               [20,6,4,1],[4,3,12,2],[15,6,8,0],[14,4,12,2]])\n",
    "\n",
    "check_transform_dtypes(model1, X1, 2)\n",
    "check_transform_dtypes(model2, X2, 2)\n",
    "check_transform_dtypes(model2, X2, 3)\n",
    "\n",
    "Y11 = model1.transform(X1, n_components=1)\n",
    "Y12 = model1.transform(X1, n_components=2)\n",
    "\n",
    "Y21 = model2.transform(X2, n_components=1)\n",
    "Y22 = model2.transform(X2, n_components=2)\n",
    "Y23 = model2.transform(X2, n_components=3)\n",
    "\n",
    "\n",
    "check_vals(Y11,np.array([[1.706],[4.4027],[1.8679]]))\n",
    "check_vals(Y12,np.array([[1.706,-1.9197],[4.4027,0.1311],[1.8679,-1.466]]))\n",
    "\n",
    "check_vals(Y21,np.array([[-3.3206],[5.2079],[-0.6142],[11.5884],\n",
    "                         [-5.3006],[11.7232],[0.1272],[1.7683]]))\n",
    "\n",
    "check_vals(Y22,np.array([[-3.3206,3.8531],[5.2079,-0.4483],[-0.6142,-1.8428],[11.5884,4.8089],\n",
    "                         [-5.3006,2.8181],[11.7232,-3.248],[0.1272,-0.5616],[1.7683,-4.3514]]))\n",
    "\n",
    "check_vals(Y23,np.array([[-3.3206,3.8531,-1.9486],[5.2079,-0.4483,2.0691],[-0.6142,-1.8428,-0.8391],\n",
    "                         [11.5884,4.8089,-0.0945],[-5.3006,2.8181,0.2617],[11.7232,-3.248,-1.1706],\n",
    "                         [0.1272,-0.5616,1.203],[1.7683,-4.3514,-0.5627]]))\n",
    "\n",
    "\n",
    "############# CHECK INVERSE ################\n",
    "\n",
    "def check_inverse_dtypes(model, X, Y):\n",
    "    X_inverse = model.inverse(Y)\n",
    "    assert isinstance(X_inverse, np.ndarray) and X.shape==X_inverse.shape\n",
    "\n",
    "check_inverse_dtypes(model1, X1, Y12)\n",
    "check_inverse_dtypes(model2, X2, Y22)\n",
    "check_inverse_dtypes(model2, X2, Y23)\n",
    "\n",
    "X_inv11 = model1.inverse(Y11)\n",
    "X_inv12 = model1.inverse(Y12)\n",
    "\n",
    "X_inv21 = model2.inverse(Y21)\n",
    "X_inv22 = model2.inverse(Y22)\n",
    "X_inv23 = model2.inverse(Y23)\n",
    "\n",
    "\n",
    "check_vals(X_inv11, np.array([[4.5612,4.6762,0.8147],[1.9707,5.1128,0.2056],[4.4057,4.7024,0.7781]]))\n",
    "check_vals(X_inv12, np.array([[4.0319,3.8052,2.4415],[2.0068,5.1723,0.0945],[4.0015,4.0373,2.0204]]))\n",
    "\n",
    "check_vals(X_inv21, np.array([[18.4005,5.163,6.9177,0.581],[9.9523,4.7444,7.9852,0.8011],\n",
    "                              [15.7195,5.0301,7.2564,0.6508],[3.6318,4.4313,8.7839,0.9658],\n",
    "                              [20.3618,5.2601,6.6698,0.5299],[3.4983,4.4246,8.8008,0.9692],\n",
    "                              [14.9851,4.9938,7.3493,0.67],[13.3595,4.9132,7.5547,0.7123]]))\n",
    "\n",
    "check_vals(X_inv22, np.array([[17.9121,5.7952,3.188,1.1269],[10.0091,4.6708,8.4192,0.7376],\n",
    "                              [15.9531,4.7278,9.0402,0.3897],[3.0223,5.2203,4.1292,1.6471],\n",
    "                              [20.0046,5.7225,3.942,0.9292],[3.91,3.8917,11.9447,0.509],\n",
    "                              [15.0563,4.9016,7.8929,0.5904],[13.911,4.1992,11.7667,0.0958]]))\n",
    "\n",
    "check_vals(X_inv23, np.array([[17.9821,3.9102,2.9191,1.5352],[9.9347,6.6724,8.7048,0.3041],\n",
    "                              [15.9833,3.9161,8.9244,0.5655],[3.0257,5.1288,4.1161,1.6669],\n",
    "                              [19.9952,5.9757,3.9781,0.8743],[3.9521,2.7594,11.7831,0.7543],\n",
    "                              [15.013,6.0654,8.0589,0.3383],[13.9313,3.6549,11.689,0.2136]]))\n",
    "\n",
    "############# CHECK MEAN SQUARE ERROR ################\n",
    "\n",
    "err11 = model1.mean_squared_error(X1, 1)\n",
    "err12 = model1.mean_squared_error(X1, 2)\n",
    "\n",
    "err21 = model2.mean_squared_error(X2, 1)\n",
    "err22 = model2.mean_squared_error(X2, 2)\n",
    "err23 = model2.mean_squared_error(X2, 3)\n",
    "\n",
    "assert err11==approx(2.2459, rel=1e-2)\n",
    "assert err12==approx(0.29489, rel=1e-2)\n",
    "\n",
    "assert err21==approx(12.5622, rel=1e-2)\n",
    "assert err22==approx(2.64839, rel=1e-2)\n",
    "assert err23==approx(1.14915, rel=1e-2)\n",
    "\n",
    "\n",
    "# TODO: student should print their names and date\n",
    "print('student name and date ')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Project Report** (10 points)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DO NOT EDIT\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "data = pd.read_csv('digits.csv',header=None).values\n",
    "X_init = data[:,1:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Question 1\n",
    "Apply PCA to the digits data, and plot variance share as a function of the number of components. Make sure to plot cumulative values, label the axis, and set the chart title. Find the minimum number of components that have a variance share greater than or equal to 90%."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DO NOT EDIT\n",
    "digits_model = PCA()\n",
    "digits_model.fit(X_init)\n",
    "\n",
    "# TODO\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Question 2\n",
    "Use the `digits_model` initialized above and apply PCA on digit 6 (index 67, in X_init) with 1, 2, 4, 8, 16, 32 components. Plot the original and the inverses. Explain the trend, and identify the minimum number of components that sufficiently reconstruct the original image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "digit = X_init[67].reshape(1,-1)\n",
    "components = [1,2,4,8,16,32]\n",
    "\n",
    "# TODO:\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Question 3\n",
    "Apply PCA on the digits data, and transform with 1, 2, 4, 8, 16, 32 components. Perform K-means clustering on the transformed data, and plot the inverses of the cluster centers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "components = [1,2,4,8,16,32]\n",
    "num_clusters = 10\n",
    "\n",
    "# TODO:\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  },
  "vscode": {
   "interpreter": {
    "hash": "bcd9bc17ffadb8b3c09124f861805f4f094648af93180b87f0218364b7d0c0de"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
